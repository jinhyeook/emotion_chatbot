import numpy as np
import cv2
import streamlit as st

from tensorflow.keras.models import load_model
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain, LLMChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts.prompt import PromptTemplate
from PIL import Image
from contants import MODEL_PATH, API_KEY


class EmotionChatService:
    def __init__(self):
        try:
            self.model = load_model(MODEL_PATH)
        except Exception as e:
            st.error(f"ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e} ğŸš¨")
            raise
        self.chatbot = self._setup_chatbot()
        self.songbot = self._setup_songbot()
        self.class_labels = {0: "í™”ë‚¨", 1: "ì¦ê±°ì›€", 2: "ë‹¹í™©í•¨", 3: "ìŠ¬í””"}

    def _setup_chatbot(self):
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            api_key=API_KEY
        )
        chat_template = """
        ë„ˆëŠ” ê³ ë„ë¡œ í›ˆë ¨ëœ ì‹¬ë¦¬ìƒë‹´ì‚¬ì•¼. ì‚¬ìš©ìê°€ ê²ªê³  ìˆëŠ” ê°ì •ì„ ì •í™•íˆ íŒŒì•…í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³µê°ê³¼ ì‹¤ì§ˆì ì¸ ë„ì›€ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ë„ˆì˜ ì—­í• ì´ì•¼.
        
        ë‹¤ìŒ ì‚¬í•­ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•´:
        1. ì‚¬ìš©ìë¥¼ "ì„ ìƒë‹˜"ì´ë¼ê³  ë¶€ë¥´ë©° ë”°ëœ»í•˜ê³  ì§„ì •ì„± ìˆëŠ” íƒœë„ë¡œ ëŒ€í™”í•´.
        2. ëŒ€ë‹µì€ ê°„ê²°í•˜ë©´ì„œë„ ëª…í™•í•˜ê³ , 50ì ë‚´ì™¸ë¡œ ì‘ì„±í•˜ë˜ ë‚´ìš©ì´ ì¶©ì‹¤í•´ì•¼ í•´.
        3. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì í•©í•œ ê°ì • ë¶„ì„ê³¼ êµ¬ì²´ì ì¸ ì¡°ì–¸ì„ ì œê³µí•´.
        4. ê°ì • ìƒíƒœì— ë”°ë¼ ë‹¤ìŒì˜ ëŒ€í™” í†¤ì„ ìœ ì§€í•´:
           - ìŠ¬í””: ìœ„ë¡œì™€ ì•ˆì •ì„ ì£¼ëŠ” ë¶€ë“œëŸ¬ìš´ ì–´ì¡°.
           - í™”ë‚¨: ì§„ì •ê³¼ ì´í•´ë¥¼ ë•ëŠ” ì°¨ë¶„í•œ ì–´ì¡°.
           - ë‹¹í™©í•¨: ì•ˆì‹¬ì‹œí‚¤ê³  ëª…í™•ì„±ì„ ì œê³µí•˜ëŠ” ì–´ì¡°.
           - ì¦ê±°ì›€: ê¸ì •ì ì´ê³  ê²©ë ¤í•˜ëŠ” ì–´ì¡°.
        5. ì‹¬ë¦¬ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” í•´ê²°ì±…ì„ ì œì•ˆí•˜ê³ , ì‹¤ì§ˆì ì¸ ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë„ë¡ ë…¸ë ¥í•´.
        
        # ì‘ë‹µ ì˜ˆì‹œ
        Q: ë„ˆë¬´ ìš°ìš¸í•´ìš”. ì•„ë¬´ê²ƒë„ í•˜ê¸° ì‹«ì–´ìš”.
        A: ì„ ìƒë‹˜, ì´ëŸ° ê°ì •ì€ ëˆ„êµ¬ë‚˜ ê²ªì„ ìˆ˜ ìˆì–´ìš”. ì ì‹œ ì‰¬ë©´ì„œ ìì‹ ì„ ëŒë´ì£¼ì„¸ìš”.

        ì´ì „ ëŒ€í™”: {history}
        ì‚¬ìš©ìì˜ ì…ë ¥: {input}
        ë‹µë³€:"""

        return ConversationChain(
            llm=llm,
            prompt=PromptTemplate(input_variables=["history", "input"], template=chat_template),
            memory=ConversationBufferMemory()
        )

    def _setup_songbot(self):
        song_llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            api_key=API_KEY
        )

        song_template = """
        ë„ˆëŠ” ì „ë¬¸ ë…¸ë˜ ì¶”ì²œ ì „ë¬¸ê°€ì•¼. ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœì— ë”°ë¼ ì ì ˆí•œ í•œêµ­ ë…¸ë˜ë¥¼ ì¶”ì²œí•˜ê³ , ë°˜ë“œì‹œ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•´ì•¼ í•´.
        
        ë‹¤ìŒ ì‚¬í•­ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•´:
        1. ì¶”ì²œ ë…¸ë˜ëŠ” ë°˜ë“œì‹œ "ë…¸ë˜ì œëª© - ê°€ìˆ˜ì´ë¦„" í˜•ì‹ìœ¼ë¡œ ì œê³µí•˜ê³ , í•´ë‹¹ ê°€ìˆ˜ê°€ ë°œí‘œí•œ ê³¡ì´ì–´ì•¼ í•´.
        2. ë…¸ë˜ë¥¼ ì¶”ì²œí•  ë•Œ ì´ìœ ë¥¼ ê°„ëµíˆ ì„¤ëª…í•˜ë˜, ì¶”ì²œ ì´ìœ ê°€ ê°ì • ìƒíƒœì™€ ëª…í™•íˆ ì—°ê´€ë˜ì–´ì•¼ í•´.
        3. ê°ì • ìƒíƒœì— ë”°ë¼ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì²œ ê³¡ì„ ì„ íƒí•´:
           - ìŠ¬í””: ìœ„ë¡œì™€ í¬ë§ì„ ì£¼ëŠ” ë°œë¼ë“œ.
           - í™”ë‚¨: ê°ì •ì„ í•´ì†Œí•  ìˆ˜ ìˆëŠ” ê°•ë ¬í•˜ê³  ì—ë„ˆì§€ ë„˜ì¹˜ëŠ” ê³¡.
           - ë‹¹í™©í•¨: ì•ˆì •ê°ì„ ì£¼ëŠ” ì°¨ë¶„í•œ ë©œë¡œë””.
           - ì¦ê±°ì›€: í™œê¸°ì°¨ê³  ê¸ì •ì ì¸ ë¶„ìœ„ê¸°ì˜ ê³¡.
        4. ì¶”ì²œì€ ì‹ ë¢°ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì˜ëª»ëœ ë§¤ì¹­ì´ ì—†ë„ë¡ ìœ ì˜í•´.
        
        # ì˜ˆì‹œ
        Q: ìŠ¬í”Œ ë•Œ ë“¤ì„ë§Œí•œ ë…¸ë˜ ì¶”ì²œí•´ì¤˜.
        A: "ë¹„ì™€ ë‹¹ì‹ ì˜ ì´ì•¼ê¸° - ë¶€í™œ"ì€ ìŠ¬í””ì„ ìœ„ë¡œí•˜ê³  í¬ë§ì„ ì¤„ ìˆ˜ ìˆëŠ” ê³¡ì´ì—ìš”.

        ì§ˆë¬¸: {question}
        ë‹µë³€:"""

        return LLMChain(
            llm=song_llm,
            prompt=PromptTemplate(input_variables=["question"], template=song_template)
        )

    # ì–¼êµ´ë¶€ë¶„ë§Œ í¬ë¡­
    def process_image(self, uploaded_file):
        try:
            img = Image.open(uploaded_file)
            img_array = np.array(img)
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            faces = face_cascade.detectMultiScale(gray, 1.1, 4)

            if len(faces) == 0:
                return None, "ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ˜¢"

            x, y, w, h = max(faces, key=lambda x: x[2] * x[3])
            y = max(y - int(h * 0.5), 0)
            h = int(h * 1.5)
            cropped_img = img_array[y:y+h, x:x+w]
            return self.predict_emotion(cropped_img), None
        except Exception as e:
            return None, f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e} âš ï¸"

    def predict_emotion(self, img_array):
        try:
            img = Image.fromarray(img_array).resize((224, 224))
            img_array = np.array(img) / 255.0
            img_array = np.expand_dims(img_array, axis=0)
            predictions = self.model.predict(img_array)
            return self.class_labels[np.argmax(predictions)]
        except Exception as e:
            return f"ì˜ˆì¸¡ ì˜¤ë¥˜: {e} âš ï¸"
